import os
import math
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass
from pathlib import Path
from typing import Tuple, List, Optional, Dict, Any
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import datasets, models, transforms
from PIL import Image
from sklearn.metrics import confusion_matrix, classification_report
from tqdm import tqdm

# ==========================================
# 1. CONFIGURATION (Type-Safe)
# ==========================================
@dataclass
class Config:
    malware_dir: Path = Path('./Dataset')
    stl10_dir: Path = Path('./data_stl10')
    ckpt_dir: Path = Path('./checkpoints')
    results_dir: Path = Path('./results')
    batch_size: int = 32
    lr: float = 0.0005
    device: torch.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    seed: int = 42
    
    def __post_init__(self):
        self.ckpt_dir.mkdir(exist_ok=True, parents=True)
        self.results_dir.mkdir(exist_ok=True, parents=True)
        torch.manual_seed(self.seed)
        np.random.seed(self.seed)

cfg = Config()

# ==========================================
# 2. UTILITIES & TRANSFORMS
# ==========================================
def binary_to_image(file_path: str, target_size: Tuple[int, int] = (224, 224)) -> Optional[Image.Image]:
    """Converts binary executable raw bytes into a grayscale image representation."""
    try:
        with open(file_path, 'rb') as f:
            data = np.frombuffer(f.read(), dtype=np.uint8)
        
        if data.size == 0: return None
        
        # Calculate optimal square size
        size = int(math.isqrt(data.size))
        if size == 0: return None
        
        img_array = data[:size*size].reshape((size, size))
        return Image.fromarray(img_array).convert('RGB').resize(target_size)
    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return None

def get_transforms(augment: bool = True) -> transforms.Compose:
    """Returns the standardization and augmentation pipeline."""
    base_transforms = [
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]
    
    if augment:
        augmentations = [
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(15),
            transforms.ColorJitter(brightness=0.2, contrast=0.2)
        ]
        return transforms.Compose(augmentations + base_transforms)
    
    return transforms.Compose(base_transforms)

# ==========================================
# 3. DATASETS
# ==========================================
class GatekeeperDataset(Dataset):
    """
    Hybrid Dataset for Stage 1:
    Label 0: Natural Images (STL-10) -> REJECT
    Label 1: Digital/Malware Images -> ACCEPT
    """
    def __init__(self, malware_ds: Dataset, stl_ds: Dataset):
        self.malware_ds = malware_ds
        self.stl_ds = stl_ds
        self.limit = min(len(malware_ds), len(stl_ds))
        
    def __len__(self) -> int:
        return self.limit * 2
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:
        if idx < self.limit:
            img, _ = self.malware_ds[idx]
            return img, 1 # Digital/Malware
        else:
            img, _ = self.stl_ds[idx - self.limit]
            return img, 0 # Natural Image

# ==========================================
# 4. MODEL FACTORY
# ==========================================
def build_model(num_classes: int) -> nn.Module:
    """Constructs a ResNet18 with a modified classification head."""
    model = models.resnet18(weights='DEFAULT')
    model.fc = nn.Linear(model.fc.in_features, num_classes)
    return model.to(cfg.device)

def load_checkpoint(model: nn.Module, path: str) -> Dict[str, Any]:
    """Loads weights into model from path. Critical for inference."""
    if not os.path.exists(path):
        raise FileNotFoundError(f"Checkpoint missing: {path}")
    
    ckpt = torch.load(path, map_location=cfg.device)
    model.load_state_dict(ckpt['model_state_dict'])
    print(f"Loaded checkpoint: {Path(path).name} (Acc: {ckpt.get('val_acc', 0):.2f}%)")
    return ckpt

# ==========================================
# 5. TRAINING ENGINE (Refactored)
# ==========================================
def run_epoch(model, loader, criterion, optimizer=None) -> Tuple[float, float]:
    """Handles a single epoch pass (Train or Validation)."""
    is_train = optimizer is not None 
    model.train() if is_train else model.eval()
    
    total_loss, correct, total = 0.0, 0, 0
    context = torch.enable_grad() if is_train else torch.no_grad()
    
    with context:
        pbar = tqdm(loader, leave=False, desc="Training" if is_train else "Validating")
        for imgs, labels in pbar:
            imgs, labels = imgs.to(cfg.device), labels.to(cfg.device)
            
            if is_train:
                optimizer.zero_grad()
                
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            
            if is_train:
                loss.backward()
                optimizer.step()
            
            total_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
            
            pbar.set_postfix(loss=loss.item())

    return total_loss / len(loader), 100 * correct / total

def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, 
                epochs: int, name: str, class_weights: torch.Tensor = None) -> nn.Module:
    """Orchestrates the training lifecycle."""
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    optimizer = optim.Adam(model.parameters(), lr=cfg.lr)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)
    
    best_acc = 0.0
    print(f"\n STARTING: {name}")

    for epoch in range(1, epochs + 1):
        t_loss, t_acc = run_epoch(model, train_loader, criterion, optimizer)
        v_loss, v_acc = run_epoch(model, val_loader, criterion)
        
        scheduler.step(v_loss)
        print(f"Ep {epoch:02d} | Train: {t_loss:.4f} ({t_acc:.1f}%) | Val: {v_loss:.4f} ({v_acc:.1f}%)")

        # Save Checkpoint
        state = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'val_acc': v_acc,
            'val_loss': v_loss
        }
        
        # Save Best
        if v_acc > best_acc:
            best_acc = v_acc
            torch.save(state, cfg.ckpt_dir / f"{name}_BEST.pth")
            print(f"  New Best Model Saved ({best_acc:.2f}%)")
            
    return model

def evaluate_model(model: nn.Module, loader: DataLoader, classes: List[str], name: str):
    """Generates and saves classification metrics."""
    model.eval()
    all_preds, all_labels = [], []
    
    with torch.no_grad():
        for imgs, labels in tqdm(loader, desc="Evaluating"):
            outputs = model(imgs.to(cfg.device))
            all_preds.extend(torch.argmax(outputs, 1).cpu().numpy())
            all_labels.extend(labels.numpy())
            
    # Matrix & Report
    cm = confusion_matrix(all_labels, all_preds)
    report = classification_report(all_labels, all_preds, target_names=classes)
    
    # Save Plot
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
    plt.title(f'{name} Confusion Matrix')
    plt.savefig(cfg.results_dir / f'{name}_CM.png')
    plt.close()
    
    with open(cfg.results_dir / f'{name}_Report.txt', 'w') as f:
        f.write(report)
    print(f" Evaluation saved for {name}")

# ==========================================
# 6. PIPELINE ORCHESTRATOR
# ==========================================
class MalwareDetectionPipeline:
    def __init__(self):
        self.malware_classes = []
        
    def prepare_data(self) -> Tuple[datasets.ImageFolder, datasets.STL10]:
        if not cfg.malware_dir.exists():
            raise FileNotFoundError(f"Dataset not found at {cfg.malware_dir}")
            
        print(" Loading Datasets...")
        mw = datasets.ImageFolder(root=str(cfg.malware_dir), transform=get_transforms(True))
        stl = datasets.STL10(root=str(cfg.stl10_dir), split='train', download=True, transform=get_transforms(True))
        
        self.malware_classes = mw.classes
        return mw, stl
    
    def _get_loaders(self, dataset: Dataset) -> Tuple[DataLoader, DataLoader]:
        train_len = int(0.8 * len(dataset))
        train, val = random_split(dataset, [train_len, len(dataset) - train_len])
        
        kw = {'batch_size': cfg.batch_size, 'num_workers': 0, 'pin_memory': True}
        return DataLoader(train, shuffle=True, **kw), DataLoader(val, shuffle=False, **kw)

    def run(self):
        mw_ds, stl_ds = self.prepare_data()
        
        # --- Stage 1: Gatekeeper (Binary vs Natural) ---
        gatekeeper_ds = GatekeeperDataset(mw_ds, stl_ds)
        train_loader, val_loader = self._get_loaders(gatekeeper_ds)
        
        model_s1 = build_model(num_classes=2)
        model_s1 = train_model(model_s1, train_loader, val_loader, 15, "Stage1_Gatekeeper")
        evaluate_model(model_s1, val_loader, ['Natural', 'Digital'], "Stage1_Gatekeeper")
        
        # --- Stage 2: Specialist (Malware Families) ---
        # Compute weights for class imbalance
        targets = [y for _, y in mw_ds]
        counts = np.bincount(targets)
        weights = torch.FloatTensor(len(targets) / (len(counts) * counts)).to(cfg.device)
        
        train_loader, val_loader = self._get_loaders(mw_ds)
        model_s2 = build_model(num_classes=len(self.malware_classes))
        model_s2 = train_model(model_s2, train_loader, val_loader, 25, "Stage2_Classifier", weights)
        evaluate_model(model_s2, val_loader, self.malware_classes, "Stage2_Classifier")
        
        print("\n   Pipeline Complete. Models saved to:", cfg.ckpt_dir)

if __name__ == "__main__":
    pipeline = MalwareDetectionPipeline()
    pipeline.run()